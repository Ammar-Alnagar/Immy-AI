## Timeline 

1. LLM in a Box implementation ✔
2. Groq implementation ✔
3. Ollama implementation (offline) ✔
4. Save logs offline, then send it to LLM in a Box with chatid ✔
5. Route between offline and online models for seamless interaction ✔
6. Using websockets for ElevenLabs API (wip)
7. Use Groq token streaming (wip)
8. Llama.cpp implementation (offline) (wip)
9. Whisper.cpp (offline) (wip)
10. Other offline TTS (wip)
11. Gather dataset for Immy offline model  ✔
12. Finetune a model for offline Immy (wip)
13. LLM in a Box streaming ? (unknown status)

